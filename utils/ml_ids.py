"""
Machine Learning-Based Intrusion Detection System
Isolation Forest kullanarak anomali tespiti yapar.
"""
import numpy as np
import time
from typing import Dict, List, Optional, Tuple
from collections import deque
from loguru import logger

try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    import joblib
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("scikit-learn bulunamadÄ±. ML-IDS devre dÄ±ÅŸÄ±.")


class FeatureExtractor:
    """
    OCPP ve CAN trafiÄŸinden feature'lar Ã§Ä±karÄ±r.
    
    Features:
    1. CAN ID
    2. Payload length
    3. Payload entropy
    4. Inter-arrival time (son mesajdan bu yana geÃ§en sÃ¼re)
    5. CAN ID frequency (son 10 saniyede kaÃ§ kez gÃ¶rÃ¼ldÃ¼)
    6. Payload byte ortalamasÄ±
    7. Payload byte std deviation
    8. Time of day (0-86400 saniye)
    9. OCPP message rate (Senaryo #2)
    10. Burst detection (ani artÄ±ÅŸ tespiti - Senaryo #2)
    """
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        
        # Son mesajlarÄ±n timestamp'leri (CAN ID bazÄ±nda)
        self.last_timestamps: Dict[int, float] = {}
        
        # CAN ID frekans kuyruÄŸu (timestamp, can_id)
        self.frequency_window: deque = deque(maxlen=1000)
        
        # Senaryo #2: OCPP mesaj rate tracking
        self.ocpp_timestamps: deque = deque(maxlen=1000)
        self.ocpp_action_counts: Dict[str, int] = {}
        
        # Senaryo #3: Sampling & energy tracking
        self.meter_values: deque = deque(maxlen=1000)  # (timestamp, value)
        self.meter_timestamps: deque = deque(maxlen=1000)
    
    def extract_can_features(
        self,
        can_id: int,
        data: List[int],
        timestamp: float
    ) -> np.ndarray:
        """CAN frame'den feature vector Ã§Ä±kar"""
        
        # Feature 1: CAN ID (normalized)
        feature_can_id = can_id / 2048.0  # Max CAN ID: 0x7FF (2048)
        
        # Feature 2: Payload length
        feature_payload_len = len(data) / 8.0  # Max: 8 byte
        
        # Feature 3: Payload entropy
        feature_entropy = self._calculate_entropy(data)
        
        # Feature 4: Inter-arrival time
        last_time = self.last_timestamps.get(can_id, timestamp - 0.1)
        feature_inter_arrival = min(timestamp - last_time, 10.0) / 10.0  # Cap at 10s
        self.last_timestamps[can_id] = timestamp
        
        # Feature 5: CAN ID frequency (son 10 saniyede)
        self.frequency_window.append((timestamp, can_id))
        recent_count = sum(1 for t, cid in self.frequency_window 
                          if cid == can_id and timestamp - t <= 10.0)
        feature_frequency = min(recent_count / 100.0, 1.0)  # Normalize
        
        # Feature 6: Payload byte ortalamasÄ±
        feature_mean = np.mean(data) / 255.0 if data else 0.0
        
        # Feature 7: Payload byte std deviation
        feature_std = (np.std(data) / 255.0) if len(data) > 1 else 0.0
        
        # Feature 8: Time of day (cyclical encoding)
        time_of_day = timestamp % 86400  # Saniye cinsinden
        feature_time_sin = np.sin(2 * np.pi * time_of_day / 86400)
        feature_time_cos = np.cos(2 * np.pi * time_of_day / 86400)
        
        # Feature vector oluÅŸtur
        features = np.array([
            feature_can_id,
            feature_payload_len,
            feature_entropy,
            feature_inter_arrival,
            feature_frequency,
            feature_mean,
            feature_std,
            feature_time_sin,
            feature_time_cos
        ])
        
        return features
    
    def extract_ocpp_rate_features(
        self,
        action: str,
        timestamp: float,
        window: float = 1.0
    ) -> Tuple[float, float]:
        """
        OCPP mesaj rate'i iÃ§in feature Ã§Ä±kar (Senaryo #2).
        
        Returns:
            (message_rate, burst_score)
            - message_rate: Son window iÃ§inde mesaj/saniye
            - burst_score: Ani artÄ±ÅŸ skoru (0-1)
        """
        # Timestamp'i kaydet
        self.ocpp_timestamps.append(timestamp)
        
        # Action count'u gÃ¼ncelle
        self.ocpp_action_counts[action] = self.ocpp_action_counts.get(action, 0) + 1
        
        # Son window iÃ§indeki mesajlarÄ± say
        recent_messages = [t for t in self.ocpp_timestamps if timestamp - t <= window]
        message_rate = len(recent_messages) / window if window > 0 else 0
        
        # Burst detection: Son 1 saniye vs Ã¶nceki 10 saniye karÅŸÄ±laÅŸtÄ±rmasÄ±
        recent_1s = sum(1 for t in self.ocpp_timestamps if timestamp - t <= 1.0)
        recent_10s = sum(1 for t in self.ocpp_timestamps if timestamp - t <= 10.0)
        
        avg_rate_10s = recent_10s / 10.0 if recent_10s > 0 else 0.1
        burst_score = min(recent_1s / (avg_rate_10s + 0.1), 10.0) / 10.0  # Normalize to [0, 1]
        
        return message_rate, burst_score
    
    def extract_sampling_features(
        self,
        meter_value: float,
        timestamp: float,
        window: float = 60.0
    ) -> Tuple[float, float, float]:
        """
        Enerji Ã¶lÃ§Ã¼m sampling iÃ§in feature Ã§Ä±kar (Senaryo #3).
        
        Returns:
            (sampling_rate, variance_score, inter_sample_time)
            - sampling_rate: sample/minute
            - variance_score: Rolling variance (normalized)
            - inter_sample_time: Son sample'dan beri geÃ§en sÃ¼re
        """
        # Timestamp ve deÄŸeri kaydet
        self.meter_values.append((timestamp, meter_value))
        self.meter_timestamps.append(timestamp)
        
        # Sampling rate: Son window iÃ§inde sample/minute
        recent_timestamps = [t for t in self.meter_timestamps if timestamp - t <= window]
        sampling_rate = len(recent_timestamps) / (window / 60.0) if window > 0 else 0
        
        # Variance score: Son N deÄŸerin varyansÄ±
        recent_values = [v for t, v in self.meter_values if timestamp - t <= window]
        variance_score = 0.0
        if len(recent_values) >= 3:
            variance = np.var(recent_values)
            # Normalize (tipik varyans: 0.1-1.0 kWh^2)
            variance_score = min(variance / 1.0, 1.0)
        
        # Inter-sample time: Son sample'dan beri geÃ§en sÃ¼re
        if len(self.meter_timestamps) >= 2:
            inter_sample_time = self.meter_timestamps[-1] - self.meter_timestamps[-2]
        else:
            inter_sample_time = 0.0
        
        # Normalize (tipik: 1-5 saniye)
        inter_sample_normalized = min(inter_sample_time / 60.0, 1.0)
        
        return sampling_rate, variance_score, inter_sample_normalized
    
    def _calculate_entropy(self, data: List[int]) -> float:
        """Shannon entropy hesapla"""
        if not data:
            return 0.0
        
        # Byte frekanslarÄ±
        counts = np.bincount(data, minlength=256)
        probabilities = counts / len(data)
        
        # Entropy
        entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])
        
        # Normalize (max entropy = 8 bits)
        return entropy / 8.0


class MLBasedIDS:
    """
    Makine Ã¶ÄŸrenmesi tabanlÄ± saldÄ±rÄ± tespit sistemi.
    Isolation Forest algoritmasÄ± kullanÄ±r.
    """
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        contamination: float = 0.1,  # Anomali oranÄ± tahmini
        threshold: float = 0.7
    ):
        if not SKLEARN_AVAILABLE:
            logger.error("sklearn kurulu deÄŸil! ML-IDS kullanÄ±lamaz.")
            self.model = None
            self.scaler = None
            return
        
        self.model_path = model_path
        self.contamination = contamination
        self.threshold = threshold
        
        self.feature_extractor = FeatureExtractor()
        self.scaler = StandardScaler()
        
        # Training buffer
        self.training_buffer: List[np.ndarray] = []
        self.is_trained = False  # VarsayÄ±lan olarak eÄŸitilmemiÅŸ
        
        # Model yÃ¼kle veya yeni oluÅŸtur
        if model_path:
            self.load_model(model_path)  # Bu is_trained = True yapacak
        else:
            self.model = IsolationForest(
                contamination=contamination,
                random_state=42,
                n_estimators=100
            )
            logger.info("Yeni Isolation Forest modeli oluÅŸturuldu (eÄŸitilmemiÅŸ)")
    
    def predict(
        self,
        can_id: int,
        data: List[int],
        timestamp: float
    ) -> Tuple[bool, float]:
        """
        CAN frame'in anomali olup olmadÄ±ÄŸÄ±nÄ± tahmin et.
        
        Returns:
            (is_anomaly, anomaly_score) tuple
            - is_anomaly: True ise anomali
            - anomaly_score: 0-1 arasÄ±, 1'e yakÄ±n = daha anormal
        """
        if self.model is None or not self.is_trained:
            return False, 0.0
        
        # Feature extraction
        features = self.feature_extractor.extract_can_features(can_id, data, timestamp)
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        
        # Prediction
        # Isolation Forest: -1 = anomaly, 1 = normal
        prediction = self.model.predict(features_scaled)[0]
        
        # Anomaly score (decision function)
        # Negatif deÄŸer = anomali, pozitif = normal
        score_raw = self.model.decision_function(features_scaled)[0]
        
        # Score'u 0-1 aralÄ±ÄŸÄ±na normalize et
        # score_raw genelde [-0.5, 0.5] arasÄ±nda
        anomaly_score = max(0.0, min(1.0, 0.5 - score_raw))
        
        is_anomaly = prediction == -1 or anomaly_score >= self.threshold
        
        if is_anomaly:
            logger.debug(f"ML-IDS: Anomali tespit edildi (score={anomaly_score:.3f})")
        
        return is_anomaly, anomaly_score
    
    def add_training_sample(
        self,
        can_id: int,
        data: List[int],
        timestamp: float
    ) -> None:
        """EÄŸitim iÃ§in Ã¶rnek ekle"""
        features = self.feature_extractor.extract_can_features(can_id, data, timestamp)
        self.training_buffer.append(features)
    
    def train(self, min_samples: int = 100) -> bool:
        """
        Modeli eÄŸit.
        
        Args:
            min_samples: Minimum eÄŸitim Ã¶rneÄŸi sayÄ±sÄ±
        
        Returns:
            BaÅŸarÄ±lÄ± mÄ±?
        """
        if self.model is None or not SKLEARN_AVAILABLE:
            logger.error("Model veya sklearn mevcut deÄŸil!")
            return False
        
        if len(self.training_buffer) < min_samples:
            logger.warning(f"Yetersiz eÄŸitim verisi: {len(self.training_buffer)}/{min_samples}")
            return False
        
        try:
            X = np.array(self.training_buffer)
            
            # Scaler'Ä± fit et
            self.scaler.fit(X)
            X_scaled = self.scaler.transform(X)
            
            # Model eÄŸit
            logger.info(f"Model eÄŸitiliyor... ({len(X)} Ã¶rnek)")
            self.model.fit(X_scaled)
            
            self.is_trained = True
            logger.info("âœ“ Model baÅŸarÄ±yla eÄŸitildi")
            
            return True
        except Exception as e:
            logger.error(f"Model eÄŸitim hatasÄ±: {e}")
            return False
    
    def save_model(self, path: str) -> bool:
        """Modeli ve scaler'Ä± kaydet"""
        if self.model is None or not self.is_trained:
            logger.error("EÄŸitilmiÅŸ model yok!")
            return False
        
        try:
            joblib.dump({
                "model": self.model,
                "scaler": self.scaler,
                "contamination": self.contamination
            }, path)
            logger.info(f"Model kaydedildi: {path}")
            return True
        except Exception as e:
            logger.error(f"Model kaydetme hatasÄ±: {e}")
            return False
    
    def load_model(self, path: str) -> bool:
        """Modeli ve scaler'Ä± yÃ¼kle"""
        try:
            data = joblib.load(path)
            self.model = data["model"]
            self.scaler = data["scaler"]
            self.contamination = data.get("contamination", 0.1)
            self.is_trained = True
            logger.info(f"Model yÃ¼klendi: {path}")
            return True
        except FileNotFoundError:
            logger.warning(f"Model dosyasÄ± bulunamadÄ±: {path}")
            return False
        except Exception as e:
            logger.error(f"Model yÃ¼kleme hatasÄ±: {e}")
            return False


class HybridIDS:
    """
    Rule-based ve ML-based IDS'i birleÅŸtirir.
    
    Karar mantÄ±ÄŸÄ±:
    - Rule-based IDS HIGH/CRITICAL alert verirse â†’ direkt alarm
    - ML-IDS anomali skoruif >= threshold â†’ alarm
    - Her ikisi de normal derse â†’ normal
    """
    
    def __init__(
        self,
        rule_based_ids,  # RuleBasedIDS instance
        ml_based_ids: Optional[MLBasedIDS] = None,
        ml_weight: float = 0.3  # ML skorunun aÄŸÄ±rlÄ±ÄŸÄ± (0-1)
    ):
        self.rule_based_ids = rule_based_ids
        self.ml_based_ids = ml_based_ids
        self.ml_weight = ml_weight
        
        # ML tespit sayacÄ±
        self.ml_detection_count = 0
        self.total_ml_checks = 0
        
        logger.info(f"Hybrid IDS baÅŸlatÄ±ldÄ± (ML weight={ml_weight})")
    
    def check_can_frame(
        self,
        can_id: int,
        data: List[int],
        timestamp: float
    ) -> Tuple[Optional[object], Optional[float]]:
        """
        CAN frame'i hem rule-based hem ML-based kontrol et.
        
        Returns:
            (Alert object or None, ML anomaly score or None)
        """
        # 1. Rule-based check
        alert = self.rule_based_ids.check_can_frame(can_id, data, timestamp)
        
        # 2. ML-based check (eÄŸer model varsa ve eÄŸitilmiÅŸse)
        ml_score = None
        if self.ml_based_ids and self.ml_based_ids.is_trained:
            self.total_ml_checks += 1
            is_ml_anomaly, ml_score = self.ml_based_ids.predict(can_id, data, timestamp)
            
            # ML anomali tespit ettiyse ve rule-based tespit etmediyse
            if is_ml_anomaly and not alert:
                self.ml_detection_count += 1
                # ML-based alert oluÅŸtur
                alert = self.rule_based_ids._create_alert(
                    alert_type="ML_ANOMALY_DETECTED",
                    severity="MEDIUM",
                    description=f"ML modeli anomali tespit etti (score={ml_score:.3f})",
                    source="ML",
                    data={
                        "can_id": hex(can_id),
                        "data": [hex(b) for b in data],
                        "ml_score": ml_score
                    }
                )
                logger.info(f"ğŸ¤– ML-IDS Alert: {alert.description}")
        
        return alert, ml_score


if __name__ == "__main__":
    # Test
    logger.info("ML-Based IDS test ediliyor...")
    
    if not SKLEARN_AVAILABLE:
        print("âŒ sklearn kurulu deÄŸil. Test yapÄ±lamÄ±yor.")
        print("Kurmak iÃ§in: pip install scikit-learn")
        exit(1)
    
    print("\n" + "="*50)
    print("FEATURE EXTRACTION TEST")
    print("="*50)
    
    extractor = FeatureExtractor()
    features = extractor.extract_can_features(
        can_id=0x200,
        data=[0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08],
        timestamp=time.time()
    )
    print(f"Feature vector boyutu: {len(features)}")
    print(f"Features: {features}")
    
    print("\n" + "="*50)
    print("ML MODEL TRAINING TEST")
    print("="*50)
    
    ml_ids = MLBasedIDS()
    
    # Normal trafik Ã¶rnekleri ekle
    print("Normal trafik Ã¶rnekleri ekleniyor...")
    for i in range(200):
        ml_ids.add_training_sample(
            can_id=0x200 + (i % 3),
            data=[i % 256, (i+1) % 256, (i+2) % 256, 0x00, 0x00, 0x00, 0x00, 0x00],
            timestamp=time.time() + i * 0.01
        )
    
    # Model eÄŸit
    success = ml_ids.train(min_samples=100)
    if success:
        print("âœ“ Model baÅŸarÄ±yla eÄŸitildi")
        
        # Test
        print("\n" + "="*50)
        print("ANOMALY PREDICTION TEST")
        print("="*50)
        
        # Normal frame
        is_anom, score = ml_ids.predict(0x200, [0x01, 0x02, 0x03, 0x00], time.time())
        print(f"\nNormal frame: anomaly={is_anom}, score={score:.3f}")
        
        # Anormal frame (tamamen farklÄ±)
        is_anom, score = ml_ids.predict(0x9FF, [0xFF] * 8, time.time())
        print(f"Anormal frame: anomaly={is_anom}, score={score:.3f}")
    else:
        print("âŒ Model eÄŸitimi baÅŸarÄ±sÄ±z")

